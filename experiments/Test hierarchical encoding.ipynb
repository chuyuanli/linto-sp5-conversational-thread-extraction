{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import logging\n",
    "from typing import Dict, List, Iterable\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from allennlp.data.dataset_readers import SnliReader\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "# to be checked: reads a text as a list of sentences\n",
    "from allennlp.data.dataset_readers import TextClassificationJsonReader\n",
    "from allennlp.data.fields import Field\n",
    "from allennlp.data.fields import LabelField\n",
    "from allennlp.data.fields import TextField, ListField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Tokenizer\n",
    "from allennlp.modules.seq2vec_encoders import BertPooler\n",
    "from allennlp.modules import TextFieldEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import BagOfEmbeddingsEncoder\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "\n",
    "\n",
    "\n",
    "from allennlp.modules import Seq2VecEncoder\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, WordTokenizer, PretrainedTransformerTokenizer\n",
    "from allennlp.data.token_indexers import PretrainedTransformerIndexer\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.common import Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dummy_chat_reader import ChatReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 1115.65it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 15917.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_fields': 3, 'list_tokens_length': 7, 'list_num_tokens': 7}\n",
      "{'num_fields': 4, 'list_tokens_length': 9, 'list_num_tokens': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "token_indexers = {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "tokenizer_cfg = Params({\"word_splitter\": {\"language\": \"en\"}})\n",
    "\n",
    "tokenizer = Tokenizer.from_params(tokenizer_cfg)\n",
    "\n",
    "\n",
    "reader = ChatReader(\n",
    "    tokenizer=tokenizer,\n",
    "    token_indexers=token_indexers,\n",
    "    )\n",
    "train_instances = reader.read(\"./train_dummy.tsv\")\n",
    "vocab = Vocabulary.from_instances(train_instances)\n",
    "\n",
    "\n",
    "for i in train_instances:\n",
    "    #print(i)\n",
    "    i[\"lines\"].index(vocab)\n",
    "    print(i[\"lines\"].get_padding_lengths())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [another, chat, starts],\n",
       " '_token_indexers': {'tokens': <allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer at 0x7f940d963450>},\n",
       " '_indexed_tokens': {'tokens': [7, 6, 12]},\n",
       " '_indexer_name_to_indexed_token': {'tokens': ['tokens']},\n",
       " '_token_index_to_indexer_name': {'tokens': 'tokens'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[\"lines\"][0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muller/anaconda3/envs/allennlp/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "400000it [00:02, 160810.00it/s]\n"
     ]
    }
   ],
   "source": [
    "turn_encoder_cfg = Params({\"type\":\"gru\",'input_size': 100, 'hidden_size': 50, 'num_layers': 1,\n",
    "                  'dropout': 0.25, 'bidirectional': False\n",
    "})\n",
    "#can be changed dynamically encoder_cfg[\"type\"] = \"lstm\"\n",
    "# warning: if bidirectional, state output dimension is hidden_size x 2 -> model doesn't know that\n",
    "\n",
    "turn_encoder = Seq2VecEncoder.from_params(turn_encoder_cfg)\n",
    "turn_encoder.hidden_size = turn_encoder_cfg[\"hidden_size\"]\n",
    "\n",
    "\n",
    "chat_encoder_cfg = Params({\"type\":\"gru\",'input_size': 50, 'hidden_size': 50, 'num_layers': 1,\n",
    "                  'dropout': 0.25, 'bidirectional': False\n",
    "})\n",
    "chat_encoder = Seq2VecEncoder.from_params(chat_encoder_cfg)\n",
    "chat_encoder.hidden_size = chat_encoder_cfg[\"hidden_size\"]\n",
    "\n",
    "\n",
    "\n",
    "glove_text_field_embedder = Embedding.from_params(vocab,Params({\"pretrained_file\": \"https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz\",\n",
    "                                                          \"embedding_dim\": 100,\n",
    "                                                          \"trainable\": False\n",
    "}))\n",
    "\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=100)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "\n",
    "\n",
    "# not used for now: type should be bucket\n",
    "trainer_cfg = Params({\"iterator\": {\"type\": \"basic\",\n",
    "                                   \"batch_size\": 32\n",
    "},\n",
    "                      \"trainer\": {\n",
    "                          \"optimizer\": {\n",
    "                              \"type\": \"adam\"\n",
    "                          },\n",
    "                          \"num_epochs\": 3,\n",
    "                          \"patience\": 10,\n",
    "                          \"cuda_device\": -1\n",
    "                      }\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hierarchical_encoder import HierarchicalChatClassification\n",
    "\n",
    "model = HierarchicalChatClassification(vocab,word_embeddings,turn_encoder,chat_encoder)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "iterator = BucketIterator(batch_size=2,sorting_keys=[(\"lines\",\"list_num_tokens\")])\n",
    "iterator.index_with(vocab)\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_instances,\n",
    "                  should_log_parameter_statistics = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0000, loss: 0.6784 ||: 100%|██████████| 1/1 [00:00<00:00, 99.30it/s]\n",
      "accuracy: 1.0000, loss: 0.5810 ||: 100%|██████████| 1/1 [00:00<00:00, 88.41it/s]\n",
      "accuracy: 1.0000, loss: 0.5029 ||: 100%|██████████| 1/1 [00:00<00:00, 112.61it/s]\n",
      "accuracy: 1.0000, loss: 0.4389 ||: 100%|██████████| 1/1 [00:00<00:00, 120.47it/s]\n",
      "accuracy: 1.0000, loss: 0.3854 ||: 100%|██████████| 1/1 [00:00<00:00, 118.41it/s]\n",
      "accuracy: 1.0000, loss: 0.3403 ||: 100%|██████████| 1/1 [00:00<00:00, 93.00it/s]\n",
      "accuracy: 1.0000, loss: 0.3018 ||: 100%|██████████| 1/1 [00:00<00:00, 108.93it/s]\n",
      "accuracy: 1.0000, loss: 0.2688 ||: 100%|██████████| 1/1 [00:00<00:00, 130.66it/s]\n",
      "accuracy: 1.0000, loss: 0.2405 ||: 100%|██████████| 1/1 [00:00<00:00, 111.60it/s]\n",
      "accuracy: 1.0000, loss: 0.2159 ||: 100%|██████████| 1/1 [00:00<00:00, 135.64it/s]\n",
      "accuracy: 1.0000, loss: 0.1946 ||: 100%|██████████| 1/1 [00:00<00:00, 96.25it/s]\n",
      "accuracy: 1.0000, loss: 0.1761 ||: 100%|██████████| 1/1 [00:00<00:00, 149.83it/s]\n",
      "accuracy: 1.0000, loss: 0.1599 ||: 100%|██████████| 1/1 [00:00<00:00, 143.95it/s]\n",
      "accuracy: 1.0000, loss: 0.1457 ||: 100%|██████████| 1/1 [00:00<00:00, 109.82it/s]\n",
      "accuracy: 1.0000, loss: 0.1333 ||: 100%|██████████| 1/1 [00:00<00:00, 111.17it/s]\n",
      "accuracy: 1.0000, loss: 0.1223 ||: 100%|██████████| 1/1 [00:00<00:00, 111.98it/s]\n",
      "accuracy: 1.0000, loss: 0.1125 ||: 100%|██████████| 1/1 [00:00<00:00, 113.88it/s]\n",
      "accuracy: 1.0000, loss: 0.1039 ||: 100%|██████████| 1/1 [00:00<00:00, 115.78it/s]\n",
      "accuracy: 1.0000, loss: 0.0962 ||: 100%|██████████| 1/1 [00:00<00:00, 106.79it/s]\n",
      "accuracy: 1.0000, loss: 0.0893 ||: 100%|██████████| 1/1 [00:00<00:00, 129.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 19,\n",
       " 'peak_cpu_memory_MB': 331.64,\n",
       " 'peak_gpu_0_memory_MB': 959,\n",
       " 'training_duration': '0:00:00.581604',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 19,\n",
       " 'epoch': 19,\n",
       " 'training_accuracy': 1.0,\n",
       " 'training_loss': 0.08934460580348969,\n",
       " 'training_cpu_memory_MB': 331.64,\n",
       " 'training_gpu_0_memory_MB': 959}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
