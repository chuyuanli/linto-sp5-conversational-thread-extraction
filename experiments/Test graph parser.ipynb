{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "\n",
    "I) version base\n",
    "- (done)  reprendre classifieur de chat hiérarchique, remplacer encodeur seq2vec par un seq2seq (stacked lstm eg)\n",
    "    cf HierarchicalChatSequenceClassification\n",
    "- (done)  ajouter séquence de labels / comme si arbre ou forêt (possible ds graph_parser ?)\n",
    "\n",
    "- sousclasser graph_parser / modifs\n",
    "- brancher données réelles\n",
    "\n",
    "/home/muller/anaconda3/envs/allennlp/lib/python3.7/site-packages/allennlp/data/dataset_readers/semantic_dependency_parsing.py\n",
    "\n",
    "\n",
    "améliorations: \n",
    "- encodeur tour -> bert\n",
    "- graphe de labels -> AdjacencyMatrixField\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import logging\n",
    "from typing import Dict, List, Iterable\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.fields import Field, LabelField, TextField, ListField, SequenceLabelField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, WordTokenizer, PretrainedTransformerTokenizer\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "\n",
    "from allennlp.modules import Seq2VecEncoder, Seq2SeqEncoder\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.common import Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dummy_chat_reader import ChatReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 1341.96it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 27235.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance with fields:\n",
      " \t lines: ListField of 3 TextFields : \n",
      " \t TextField of length 7 with text: \n",
      " \t\t[this, is, a, turn, in, a, chat]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
      " \t TextField of length 2 with text: \n",
      " \t\t[another, one]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
      " \t TextField of length 4 with text: \n",
      " \t\t[but, this, is, different]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
      " \n",
      " \t label: SequenceLabelField of length 3 with labels:\n",
      " \t\t['0', '0', '0']\n",
      " \t\tin namespace: 'labels'. \n",
      "\n",
      "{'num_fields': 3, 'list_tokens_length': 7, 'list_num_tokens': 7}\n",
      "Instance with fields:\n",
      " \t lines: ListField of 4 TextFields : \n",
      " \t TextField of length 3 with text: \n",
      " \t\t[another, chat, starts]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
      " \t TextField of length 4 with text: \n",
      " \t\t[is, it, different, ?]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
      " \t TextField of length 4 with text: \n",
      " \t\t[well, a, little, bit]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
      " \t TextField of length 9 with text: \n",
      " \t\t[but, we, need, a, different, number, of, turns, .]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
      " \n",
      " \t label: SequenceLabelField of length 4 with labels:\n",
      " \t\t['0', '0', '1', '1']\n",
      " \t\tin namespace: 'labels'. \n",
      "\n",
      "{'num_fields': 4, 'list_tokens_length': 9, 'list_num_tokens': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "token_indexers = {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "tokenizer_cfg = Params({\"word_splitter\": {\"language\": \"en\"}})\n",
    "\n",
    "tokenizer = Tokenizer.from_params(tokenizer_cfg)\n",
    "\n",
    "\n",
    "reader = ChatReader(\n",
    "    tokenizer=tokenizer,\n",
    "    token_indexers=token_indexers,\n",
    "    )\n",
    "train_instances = reader.read(\"./train_dummy.tsv\")\n",
    "vocab = Vocabulary.from_instances(train_instances)\n",
    "\n",
    "\n",
    "for i in train_instances:\n",
    "    print(i)\n",
    "    i[\"lines\"].index(vocab)\n",
    "    i[\"label\"].index(vocab)\n",
    "    print(i[\"lines\"].get_padding_lengths())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance with fields:\n",
      " \t lines: ListField of 4 TextFields : \n",
      " \t TextField of length 3 with text: \n",
      " \t\t[another, chat, starts]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
      " \t TextField of length 4 with text: \n",
      " \t\t[is, it, different, ?]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
      " \t TextField of length 4 with text: \n",
      " \t\t[well, a, little, bit]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
      " \t TextField of length 9 with text: \n",
      " \t\t[but, we, need, a, different, number, of, turns, .]\n",
      " \t\tand TokenIndexers : {'tokens': 'SingleIdTokenIndexer'} \n",
      " \n",
      " \t label: SequenceLabelField of length 4 with labels:\n",
      " \t\t['0', '0', '1', '1']\n",
      " \t\tin namespace: 'labels'. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muller/anaconda3/envs/allennlp/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "400000it [00:02, 157953.90it/s]\n"
     ]
    }
   ],
   "source": [
    "turn_encoder_cfg = Params({\"type\":\"gru\",'input_size': 100, 'hidden_size': 50, 'num_layers': 1,\n",
    "                  'dropout': 0.25, 'bidirectional': False\n",
    "})\n",
    "#can be changed dynamically encoder_cfg[\"type\"] = \"lstm\"\n",
    "# warning: if bidirectional, state output dimension is hidden_size x 2 -> model doesn't know that\n",
    "\n",
    "turn_encoder = Seq2VecEncoder.from_params(turn_encoder_cfg)\n",
    "turn_encoder.hidden_size = turn_encoder_cfg[\"hidden_size\"]\n",
    "\n",
    "\n",
    "chat_encoder_cfg = Params({\"type\":\"gru\",'input_size': 50, 'hidden_size': 50, 'num_layers': 1,\n",
    "                  'dropout': 0.25, 'bidirectional': False\n",
    "})\n",
    "chat_encoder = Seq2SeqEncoder.from_params(chat_encoder_cfg)\n",
    "chat_encoder.hidden_size = chat_encoder_cfg[\"hidden_size\"]\n",
    "\n",
    "\n",
    "\n",
    "glove_text_field_embedder = Embedding.from_params(vocab,Params({\"pretrained_file\": \"https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz\",\n",
    "                                                          \"embedding_dim\": 100,\n",
    "                                                          \"trainable\": False\n",
    "}))\n",
    "\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=100)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PytorchSeq2SeqWrapper(\n",
       "  (_module): GRU(50, 50, batch_first=True, dropout=0.25)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from biaffine parser, another config (not used)\n",
    "chat_encoder_cfg =  {\n",
    "            \"type\": \"stacked_bidirectional_lstm\",\n",
    "            \"hidden_size\": 400,\n",
    "            \"input_size\": 200,\n",
    "            \"num_layers\": 3,\n",
    "            \"recurrent_dropout_probability\": 0.3,\n",
    "            \"use_highway\": True\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "from typing import Dict, List, Iterable\n",
    "from allennlp.modules import TimeDistributed\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "\n",
    "\n",
    "class HierarchicalChatSequenceClassification(Model):\n",
    "\n",
    "    # complicated way of doing it: better to put turn_encoder inside a curtom textfield embedder\n",
    "    def __init__(self, vocab, embedder: TextFieldEmbedder, turn_encoder: Seq2VecEncoder, chat_encoder: Seq2VecEncoder):\n",
    "        super(HierarchicalChatSequenceClassification, self).__init__(vocab)\n",
    "        # turn encoder has to distribute over turns of a chat instance\n",
    "        self.turn_encoder = TimeDistributed(turn_encoder)\n",
    "        self.chat_encoder = chat_encoder\n",
    "        self.text_embedder = embedder\n",
    "        self.num_classes = self.vocab.get_vocab_size(\"label\")\n",
    "        self.tag_projection_layer = TimeDistributed(\n",
    "            torch.nn.Linear(self.chat_encoder.get_output_dim(), self.num_classes)\n",
    "        )\n",
    "        \n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        self._loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self,\n",
    "                lines,\n",
    "                label = None):\n",
    "        \n",
    "        #breakpoint()\n",
    "        # mask for each turn of each chat of the batch: shape = (batch_size x max_turns x tokens)\n",
    "        mask = get_text_field_mask(lines,num_wrapping_dims=1)\n",
    "\n",
    "        # chat turns fetching embedding\n",
    "        # turns_embedding tensor is (batch_size x turns x max tokens x token embedding size)\n",
    "        turns_embeddings = self.text_embedder(lines,num_wrapping_dims=1)\n",
    "        \n",
    "        # encoding turns\n",
    "        # turn_h has shape (batch_size x turns x encoder_output_size) eg (1x3x50)\n",
    "        turn_h = self.turn_encoder(turns_embeddings,mask)\n",
    "        \n",
    "        # encoding chat \n",
    "        # mask for chats is now nb of turns; beware weird return type of torch.max (tuple) \n",
    "        chat_mask = mask.max(axis=2)[0]\n",
    "        # chat_h is all states for chat and has shape (batch_size x turns x encoder_output_size)\n",
    "        chat_h = self.chat_encoder(turn_h,chat_mask)\n",
    "        \n",
    "        # the following is taken from simpletagger\n",
    "        batch_size, sequence_length, _ = chat_h.size()\n",
    "        # logits has shape (batchsize x turn nb x num classes)\n",
    "        logits = self.tag_projection_layer(chat_h)\n",
    "        reshaped_log_probs = logits.view(-1, self.num_classes)\n",
    "        class_probabilities = torch.nn.functional.softmax(reshaped_log_probs, dim=-1).view(\n",
    "            [batch_size, sequence_length, self.num_classes]\n",
    "        )\n",
    "\n",
    "        output_dict = {\"logits\": logits, \"class_probabilities\": class_probabilities}\n",
    "\n",
    "        #breakpoint()\n",
    "        if label is not None:\n",
    "            tag_mask = chat_mask\n",
    "            loss = sequence_cross_entropy_with_logits(logits, label, tag_mask)\n",
    "            self.accuracy(logits, label, tag_mask)\n",
    "            output_dict[\"loss\"] = loss\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "\n",
    "    # expected by trainer ??\n",
    "    #def get_parameters_for_histogram_tensorboard_logging(self,*args,**kwargs):\n",
    "    #    return []\n",
    "    \n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HierarchicalChatSequenceClassification(vocab,word_embeddings,turn_encoder,chat_encoder)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "iterator = BucketIterator(batch_size=2,sorting_keys=[(\"lines\",\"list_num_tokens\")])\n",
    "iterator.index_with(vocab)\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_instances,\n",
    "                  should_log_parameter_statistics = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8571, loss: 0.6847 ||: 100%|██████████| 1/1 [00:00<00:00, 97.83it/s]\n",
      "accuracy: 0.7143, loss: 0.6688 ||: 100%|██████████| 1/1 [00:00<00:00, 107.75it/s]\n",
      "accuracy: 0.7143, loss: 0.6551 ||: 100%|██████████| 1/1 [00:00<00:00, 127.01it/s]\n",
      "accuracy: 0.7143, loss: 0.6434 ||: 100%|██████████| 1/1 [00:00<00:00, 125.49it/s]\n",
      "accuracy: 0.7143, loss: 0.6334 ||: 100%|██████████| 1/1 [00:00<00:00, 97.85it/s]\n",
      "accuracy: 0.7143, loss: 0.6247 ||: 100%|██████████| 1/1 [00:00<00:00, 112.62it/s]\n",
      "accuracy: 0.7143, loss: 0.6172 ||: 100%|██████████| 1/1 [00:00<00:00, 99.51it/s]\n",
      "accuracy: 0.7143, loss: 0.6107 ||: 100%|██████████| 1/1 [00:00<00:00, 116.93it/s]\n",
      "accuracy: 0.7143, loss: 0.6050 ||: 100%|██████████| 1/1 [00:00<00:00, 110.02it/s]\n",
      "accuracy: 0.7143, loss: 0.6000 ||: 100%|██████████| 1/1 [00:00<00:00, 121.36it/s]\n",
      "accuracy: 0.7143, loss: 0.5957 ||: 100%|██████████| 1/1 [00:00<00:00, 85.54it/s]\n",
      "accuracy: 0.7143, loss: 0.5918 ||: 100%|██████████| 1/1 [00:00<00:00, 92.20it/s]\n",
      "accuracy: 0.7143, loss: 0.5884 ||: 100%|██████████| 1/1 [00:00<00:00, 88.34it/s]\n",
      "accuracy: 0.7143, loss: 0.5854 ||: 100%|██████████| 1/1 [00:00<00:00, 99.76it/s]\n",
      "accuracy: 0.7143, loss: 0.5826 ||: 100%|██████████| 1/1 [00:00<00:00, 91.43it/s]\n",
      "accuracy: 0.7143, loss: 0.5802 ||: 100%|██████████| 1/1 [00:00<00:00, 107.38it/s]\n",
      "accuracy: 0.7143, loss: 0.5780 ||: 100%|██████████| 1/1 [00:00<00:00, 109.66it/s]\n",
      "accuracy: 0.7143, loss: 0.5759 ||: 100%|██████████| 1/1 [00:00<00:00, 91.89it/s]\n",
      "accuracy: 0.7143, loss: 0.5741 ||: 100%|██████████| 1/1 [00:00<00:00, 113.21it/s]\n",
      "accuracy: 0.7143, loss: 0.5724 ||: 100%|██████████| 1/1 [00:00<00:00, 113.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 19,\n",
       " 'peak_cpu_memory_MB': 334.952,\n",
       " 'peak_gpu_0_memory_MB': 959,\n",
       " 'training_duration': '0:00:00.594361',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 19,\n",
       " 'epoch': 19,\n",
       " 'training_accuracy': 0.7142857142857143,\n",
       " 'training_loss': 0.5723774433135986,\n",
       " 'training_cpu_memory_MB': 334.952,\n",
       " 'training_gpu_0_memory_MB': 959}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocabulary with namespaces:  tokens, Size: 24 || labels, Size: 2 || Non Padded Namespaces: {'*tags', '*labels'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
