{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import logging\n",
    "from typing import Dict, List, Iterable\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dummy_chat_reader import ChatReader, SimpleChatReader\n",
    "from simple_encoder import ChatClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from allennlp.data.dataset_readers import SnliReader\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "# to be checked: reads a text as a list of sentences\n",
    "from allennlp.data.dataset_readers import TextClassificationJsonReader\n",
    "from allennlp.data.fields import Field\n",
    "from allennlp.data.fields import LabelField\n",
    "from allennlp.data.fields import TextField, ListField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Tokenizer\n",
    "from allennlp.modules.seq2vec_encoders import BertPooler\n",
    "from allennlp.modules import TextFieldEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import BagOfEmbeddingsEncoder\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "\n",
    "\n",
    "\n",
    "from allennlp.modules import Seq2VecEncoder\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, WordTokenizer, PretrainedTransformerTokenizer\n",
    "from allennlp.data.token_indexers import PretrainedTransformerIndexer\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.common import Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.common.from_params:instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {'word_splitter': {'language': 'en'}} and extras set()\n",
      "INFO:allennlp.common.params:type = word\n",
      "INFO:allennlp.common.from_params:instantiating class <class 'allennlp.data.tokenizers.word_tokenizer.WordTokenizer'> from params {'word_splitter': {'language': 'en'}} and extras set()\n",
      "INFO:allennlp.common.from_params:instantiating class <class 'allennlp.data.tokenizers.word_splitter.WordSplitter'> from params {'language': 'en'} and extras set()\n",
      "INFO:allennlp.common.params:word_splitter.type = spacy\n",
      "INFO:allennlp.common.from_params:instantiating class <class 'allennlp.data.tokenizers.word_splitter.SpacyWordSplitter'> from params {'language': 'en'} and extras set()\n",
      "INFO:allennlp.common.params:word_splitter.language = en\n",
      "INFO:allennlp.common.params:word_splitter.pos_tags = False\n",
      "INFO:allennlp.common.params:word_splitter.parse = False\n",
      "INFO:allennlp.common.params:word_splitter.ner = False\n",
      "INFO:allennlp.common.params:word_splitter.keep_spacy_tokens = False\n",
      "INFO:allennlp.common.params:word_splitter.split_on_spaces = False\n",
      "INFO:allennlp.common.params:start_tokens = None\n",
      "INFO:allennlp.common.params:end_tokens = None\n",
      "0it [00:00, ?it/s]INFO:dummy_chat_reader:Reading instances from lines in file at: ./train_dummy.tsv\n",
      "2it [00:00, 1107.55it/s]\n",
      "INFO:allennlp.data.vocabulary:Fitting token dictionary from dataset.\n",
      "100%|██████████| 2/2 [00:00<00:00, 26630.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens_length': 13, 'num_tokens': 13}\n",
      "{'tokens_length': 11, 'num_tokens': 11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "token_indexers = {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "tokenizer_cfg = Params({\"word_splitter\": {\"language\": \"en\"}})\n",
    "\n",
    "tokenizer = Tokenizer.from_params(tokenizer_cfg)\n",
    "\n",
    "\n",
    "reader = SimpleChatReader(\n",
    "    tokenizer=tokenizer,\n",
    "    token_indexers=token_indexers,\n",
    "    )\n",
    "train_instances = reader.read(\"./train_dummy.tsv\")\n",
    "vocab = Vocabulary.from_instances(train_instances)\n",
    "\n",
    "\n",
    "for i in train_instances:\n",
    "    #print(i)\n",
    "    i[\"sentence\"].index(vocab)\n",
    "    print(i[\"sentence\"].get_padding_lengths())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.common.from_params:instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'type': 'gru', 'input_size': 100, 'hidden_size': 50, 'num_layers': 1, 'dropout': 0.25, 'bidirectional': False} and extras set()\n",
      "INFO:allennlp.common.params:type = gru\n",
      "INFO:allennlp.common.params:batch_first = True\n",
      "INFO:allennlp.common.params:Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "INFO:allennlp.common.params:CURRENTLY DEFINED PARAMETERS: \n",
      "INFO:allennlp.common.params:input_size = 100\n",
      "INFO:allennlp.common.params:hidden_size = 50\n",
      "INFO:allennlp.common.params:num_layers = 1\n",
      "INFO:allennlp.common.params:dropout = 0.25\n",
      "INFO:allennlp.common.params:bidirectional = False\n",
      "INFO:allennlp.common.params:batch_first = True\n",
      "/home/muller/anaconda3/envs/allennlp/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.25 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "INFO:allennlp.common.params:num_embeddings = None\n",
      "INFO:allennlp.common.params:vocab_namespace = tokens\n",
      "INFO:allennlp.common.params:embedding_dim = 100\n",
      "INFO:allennlp.common.params:pretrained_file = https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz\n",
      "INFO:allennlp.common.params:projection_dim = None\n",
      "INFO:allennlp.common.params:trainable = False\n",
      "INFO:allennlp.common.params:padding_index = None\n",
      "INFO:allennlp.common.params:max_norm = None\n",
      "INFO:allennlp.common.params:norm_type = 2.0\n",
      "INFO:allennlp.common.params:scale_grad_by_freq = False\n",
      "INFO:allennlp.common.params:sparse = False\n",
      "INFO:allennlp.modules.token_embedders.embedding:Reading pretrained embeddings from file\n",
      "400000it [00:02, 162253.78it/s]\n",
      "INFO:allennlp.modules.token_embedders.embedding:Initializing pre-trained embedding layer\n",
      "INFO:allennlp.modules.token_embedders.embedding:Pretrained embeddings were found for 16 out of 18 tokens\n"
     ]
    }
   ],
   "source": [
    "encoder_cfg = Params({\"type\":\"gru\",'input_size': 100, 'hidden_size': 50, 'num_layers': 1,\n",
    "                  'dropout': 0.25, 'bidirectional': False\n",
    "})\n",
    "#can be changed dynamically encoder_cfg[\"type\"] = \"lstm\"\n",
    "# warning: if bidirectional, state output dimension is hidden_size x 2 -> model doesn't know that\n",
    "\n",
    "encoder = Seq2VecEncoder.from_params(encoder_cfg)\n",
    "encoder.hidden_size = encoder_cfg[\"hidden_size\"]\n",
    "\n",
    "\n",
    "\n",
    "glove_text_field_embedder = Embedding.from_params(vocab,Params({\"pretrained_file\": \"https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz\",\n",
    "                                                          \"embedding_dim\": 100,\n",
    "                                                          \"trainable\": False\n",
    "}))\n",
    "\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=100)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "\n",
    "\n",
    "#text_field_embedder= TextFieldEmbedder.from_params(text_field_embedder_cfg,vocab=vocab)\n",
    "# \"\"\"You need to be sure that the TextFieldEmbedder is expecting the same thing that your DatasetReader is producing, but that happens in the configuration file, and we'll talk about it later.\"\"\"\n",
    "\n",
    "\n",
    "trainer_cfg = Params({\"iterator\": {\"type\": \"basic\",\n",
    "                                   \"batch_size\": 32\n",
    "},\n",
    "                      \"trainer\": {\n",
    "                          \"optimizer\": {\n",
    "                              \"type\": \"adam\"\n",
    "                          },\n",
    "                          \"num_epochs\": 3,\n",
    "                          \"patience\": 10,\n",
    "                          \"cuda_device\": -1\n",
    "                      }\n",
    "})\n",
    "\n",
    "\n",
    "model = ChatClassification(vocab,word_embeddings,encoder)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "iterator = BucketIterator(batch_size=1,sorting_keys=[(\"sentence\",\"num_tokens\")])\n",
    "iterator.index_with(vocab)\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_instances,\n",
    "                  should_log_parameter_statistics = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Beginning training.\n",
      "INFO:allennlp.training.trainer:Epoch 0/19\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 320.396\n",
      "INFO:allennlp.training.trainer:GPU 0 memory usage MB: 959\n",
      "INFO:allennlp.training.trainer:Training\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:forward pass: turn encodings done\n",
      "INFO:root:forward pass: turn encodings done\n",
      "accuracy: 0.5000, loss: 0.6666 ||: 100%|██████████| 2/2 [00:00<00:00, 128.21it/s]\n",
      "INFO:allennlp.training.tensorboard_writer:                    Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:loss            |     0.667  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy        |     0.500  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:gpu_0_memory_MB |   959.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB   |   320.396  |       N/A\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:00:00.052596\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:00:01\n",
      "INFO:allennlp.training.trainer:Epoch 1/19\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 330.632\n",
      "INFO:allennlp.training.trainer:GPU 0 memory usage MB: 959\n",
      "INFO:allennlp.training.trainer:Training\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:forward pass: turn encodings done\n",
      "INFO:root:forward pass: turn encodings done\n",
      "accuracy: 1.0000, loss: 0.4850 ||: 100%|██████████| 2/2 [00:00<00:00, 163.79it/s]\n",
      "INFO:allennlp.training.tensorboard_writer:                    Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:loss            |     0.485  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:gpu_0_memory_MB |   959.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB   |   330.632  |       N/A\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:00:00.032887\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:00:00\n",
      "INFO:allennlp.training.trainer:Epoch 2/19\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 330.632\n",
      "INFO:allennlp.training.trainer:GPU 0 memory usage MB: 959\n",
      "INFO:allennlp.training.trainer:Training\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:forward pass: turn encodings done\n",
      "INFO:root:forward pass: turn encodings done\n",
      "accuracy: 1.0000, loss: 0.3651 ||: 100%|██████████| 2/2 [00:00<00:00, 168.43it/s]\n",
      "INFO:allennlp.training.tensorboard_writer:                    Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:loss            |     0.365  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:gpu_0_memory_MB |   959.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB   |   330.632  |       N/A\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:00:00.032642\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:00:00\n",
      "INFO:allennlp.training.trainer:Epoch 3/19\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 330.632\n",
      "INFO:allennlp.training.trainer:GPU 0 memory usage MB: 959\n",
      "INFO:allennlp.training.trainer:Training\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:forward pass: turn encodings done\n",
      "INFO:root:forward pass: turn encodings done\n",
      "accuracy: 1.0000, loss: 0.2814 ||: 100%|██████████| 2/2 [00:00<00:00, 165.78it/s]\n",
      "INFO:allennlp.training.tensorboard_writer:                    Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:loss            |     0.281  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:gpu_0_memory_MB |   959.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB   |   330.632  |       N/A\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:00:00.033217\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:00:00\n",
      "INFO:allennlp.training.trainer:Epoch 4/19\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 330.632\n",
      "INFO:allennlp.training.trainer:GPU 0 memory usage MB: 959\n",
      "INFO:allennlp.training.trainer:Training\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:forward pass: turn encodings done\n",
      "INFO:root:forward pass: turn encodings done\n",
      "accuracy: 1.0000, loss: 0.2212 ||: 100%|██████████| 2/2 [00:00<00:00, 162.80it/s]\n",
      "INFO:allennlp.training.tensorboard_writer:                    Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:loss            |     0.221  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:gpu_0_memory_MB |   959.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB   |   330.632  |       N/A\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:00:00.033698\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:00:00\n",
      "INFO:allennlp.training.trainer:Epoch 5/19\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 330.632\n",
      "INFO:allennlp.training.trainer:GPU 0 memory usage MB: 959\n",
      "INFO:allennlp.training.trainer:Training\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:forward pass: turn encodings done\n",
      "INFO:root:forward pass: turn encodings done\n",
      "accuracy: 1.0000, loss: 0.1772 ||: 100%|██████████| 2/2 [00:00<00:00, 148.23it/s]\n",
      "INFO:allennlp.training.tensorboard_writer:                    Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:loss            |     0.177  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:gpu_0_memory_MB |   959.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB   |   330.632  |       N/A\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:00:00.037022\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:00:00\n",
      "INFO:allennlp.training.trainer:Epoch 6/19\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 331.0\n",
      "INFO:allennlp.training.trainer:GPU 0 memory usage MB: 959\n",
      "INFO:allennlp.training.trainer:Training\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:forward pass: turn encodings done\n",
      "INFO:root:forward pass: turn encodings done\n",
      "accuracy: 1.0000, loss: 0.1445 ||: 100%|██████████| 2/2 [00:00<00:00, 171.69it/s]\n",
      "INFO:allennlp.training.tensorboard_writer:                    Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:loss            |     0.145  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:gpu_0_memory_MB |   959.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB   |   331.000  |       N/A\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:00:00.033894\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:00:00\n",
      "INFO:allennlp.training.trainer:Epoch 7/19\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 331.008\n",
      "INFO:allennlp.training.trainer:GPU 0 memory usage MB: 959\n",
      "INFO:allennlp.training.trainer:Training\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:forward pass: turn encodings done\n",
      "INFO:root:forward pass: turn encodings done\n",
      "accuracy: 1.0000, loss: 0.1198 ||: 100%|██████████| 2/2 [00:00<00:00, 185.62it/s]\n",
      "INFO:allennlp.training.tensorboard_writer:                    Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:loss            |     0.120  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:gpu_0_memory_MB |   959.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB   |   331.008  |       N/A\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:00:00.033149\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:00:00\n",
      "INFO:allennlp.training.trainer:Epoch 8/19\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 331.008\n",
      "INFO:allennlp.training.trainer:GPU 0 memory usage MB: 959\n",
      "INFO:allennlp.training.trainer:Training\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:forward pass: turn encodings done\n",
      "INFO:root:forward pass: turn encodings done\n",
      "accuracy: 1.0000, loss: 0.1008 ||: 100%|██████████| 2/2 [00:00<00:00, 137.96it/s]\n",
      "INFO:allennlp.training.tensorboard_writer:                    Training |  Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.tensorboard_writer:loss            |     0.101  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:gpu_0_memory_MB |   959.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB   |   331.008  |       N/A\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:00:00.034979\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:00:00\n",
      "INFO:allennlp.training.trainer:Epoch 9/19\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 331.008\n",
      "INFO:allennlp.training.trainer:GPU 0 memory usage MB: 959\n",
      "INFO:allennlp.training.trainer:Training\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:forward pass: turn encodings done\n",
      "INFO:root:forward pass: turn encodings done\n",
      "accuracy: 1.0000, loss: 0.0859 ||: 100%|██████████| 2/2 [00:00<00:00, 167.72it/s]\n",
      "INFO:allennlp.training.tensorboard_writer:                    Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:loss            |     0.086  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:gpu_0_memory_MB |   959.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB   |   331.008  |       N/A\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:00:00.033824\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:00:00\n",
      "INFO:allennlp.training.trainer:Epoch 10/19\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 331.008\n",
      "INFO:allennlp.training.trainer:GPU 0 memory usage MB: 959\n",
      "INFO:allennlp.training.trainer:Training\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:forward pass: turn encodings done\n",
      "INFO:root:forward pass: turn encodings done\n",
      "accuracy: 1.0000, loss: 0.0742 ||: 100%|██████████| 2/2 [00:00<00:00, 168.53it/s]\n",
      "INFO:allennlp.training.tensorboard_writer:                    Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:loss            |     0.074  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:gpu_0_memory_MB |   959.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB   |   331.008  |       N/A\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:00:00.032630\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:00:00\n",
      "INFO:allennlp.training.trainer:Epoch 11/19\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 331.008\n",
      "INFO:allennlp.training.trainer:GPU 0 memory usage MB: 959\n",
      "INFO:allennlp.training.trainer:Training\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:forward pass: turn encodings done\n",
      "INFO:root:forward pass: turn encodings done\n",
      "accuracy: 1.0000, loss: 0.0647 ||: 100%|██████████| 2/2 [00:00<00:00, 170.79it/s]\n",
      "INFO:allennlp.training.tensorboard_writer:                    Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:loss            |     0.065  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:gpu_0_memory_MB |   959.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB   |   331.008  |       N/A\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:00:00.031892\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:00:00\n",
      "INFO:allennlp.training.trainer:Epoch 12/19\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 331.008\n",
      "INFO:allennlp.training.trainer:GPU 0 memory usage MB: 959\n",
      "INFO:allennlp.training.trainer:Training\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:forward pass: turn encodings done\n",
      "INFO:root:forward pass: turn encodings done\n",
      "accuracy: 1.0000, loss: 0.0570 ||: 100%|██████████| 2/2 [00:00<00:00, 150.88it/s]\n",
      "INFO:allennlp.training.tensorboard_writer:                    Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:loss            |     0.057  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:gpu_0_memory_MB |   959.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB   |   331.008  |       N/A\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:00:00.036583\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:00:00\n",
      "INFO:allennlp.training.trainer:Epoch 13/19\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 331.012\n",
      "INFO:allennlp.training.trainer:GPU 0 memory usage MB: 959\n",
      "INFO:allennlp.training.trainer:Training\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:forward pass: turn encodings done\n",
      "INFO:root:forward pass: turn encodings done\n",
      "accuracy: 1.0000, loss: 0.0507 ||: 100%|██████████| 2/2 [00:00<00:00, 174.27it/s]\n",
      "INFO:allennlp.training.tensorboard_writer:                    Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:loss            |     0.051  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:gpu_0_memory_MB |   959.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB   |   331.012  |       N/A\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:00:00.034522\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:00:00\n",
      "INFO:allennlp.training.trainer:Epoch 14/19\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 331.02\n",
      "INFO:allennlp.training.trainer:GPU 0 memory usage MB: 959\n",
      "INFO:allennlp.training.trainer:Training\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:forward pass: turn encodings done\n",
      "INFO:root:forward pass: turn encodings done\n",
      "accuracy: 1.0000, loss: 0.0454 ||: 100%|██████████| 2/2 [00:00<00:00, 155.71it/s]\n",
      "INFO:allennlp.training.tensorboard_writer:                    Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:loss            |     0.045  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:gpu_0_memory_MB |   959.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB   |   331.020  |       N/A\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:00:00.034243\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:00:00\n",
      "INFO:allennlp.training.trainer:Epoch 15/19\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 331.02\n",
      "INFO:allennlp.training.trainer:GPU 0 memory usage MB: 959\n",
      "INFO:allennlp.training.trainer:Training\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:forward pass: turn encodings done\n",
      "INFO:root:forward pass: turn encodings done\n",
      "accuracy: 1.0000, loss: 0.0409 ||: 100%|██████████| 2/2 [00:00<00:00, 140.90it/s]\n",
      "INFO:allennlp.training.tensorboard_writer:                    Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:loss            |     0.041  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:gpu_0_memory_MB |   959.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB   |   331.020  |       N/A\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:00:00.037290\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:00:00\n",
      "INFO:allennlp.training.trainer:Epoch 16/19\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 331.02\n",
      "INFO:allennlp.training.trainer:GPU 0 memory usage MB: 959\n",
      "INFO:allennlp.training.trainer:Training\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:forward pass: turn encodings done\n",
      "INFO:root:forward pass: turn encodings done\n",
      "accuracy: 1.0000, loss: 0.0371 ||: 100%|██████████| 2/2 [00:00<00:00, 136.45it/s]\n",
      "INFO:allennlp.training.tensorboard_writer:                    Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:loss            |     0.037  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:gpu_0_memory_MB |   959.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB   |   331.020  |       N/A\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:00:00.038529\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:00:00\n",
      "INFO:allennlp.training.trainer:Epoch 17/19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 331.024\n",
      "INFO:allennlp.training.trainer:GPU 0 memory usage MB: 959\n",
      "INFO:allennlp.training.trainer:Training\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:forward pass: turn encodings done\n",
      "INFO:root:forward pass: turn encodings done\n",
      "accuracy: 1.0000, loss: 0.0339 ||: 100%|██████████| 2/2 [00:00<00:00, 149.54it/s]\n",
      "INFO:allennlp.training.tensorboard_writer:                    Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:loss            |     0.034  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:gpu_0_memory_MB |   959.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB   |   331.024  |       N/A\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:00:00.036496\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:00:00\n",
      "INFO:allennlp.training.trainer:Epoch 18/19\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 331.024\n",
      "INFO:allennlp.training.trainer:GPU 0 memory usage MB: 959\n",
      "INFO:allennlp.training.trainer:Training\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:forward pass: turn encodings done\n",
      "INFO:root:forward pass: turn encodings done\n",
      "accuracy: 1.0000, loss: 0.0311 ||: 100%|██████████| 2/2 [00:00<00:00, 171.30it/s]\n",
      "INFO:allennlp.training.tensorboard_writer:                    Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:loss            |     0.031  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:gpu_0_memory_MB |   959.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB   |   331.024  |       N/A\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:00:00.034861\n",
      "INFO:allennlp.training.trainer:Estimated training time remaining: 0:00:00\n",
      "INFO:allennlp.training.trainer:Epoch 19/19\n",
      "INFO:allennlp.training.trainer:Peak CPU memory usage MB: 331.024\n",
      "INFO:allennlp.training.trainer:GPU 0 memory usage MB: 959\n",
      "INFO:allennlp.training.trainer:Training\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]INFO:root:forward pass: turn encodings done\n",
      "INFO:root:forward pass: turn encodings done\n",
      "accuracy: 1.0000, loss: 0.0286 ||: 100%|██████████| 2/2 [00:00<00:00, 138.04it/s]\n",
      "INFO:allennlp.training.tensorboard_writer:                    Training |  Validation\n",
      "INFO:allennlp.training.tensorboard_writer:loss            |     0.029  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:accuracy        |     1.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:gpu_0_memory_MB |   959.000  |       N/A\n",
      "INFO:allennlp.training.tensorboard_writer:cpu_memory_MB   |   331.024  |       N/A\n",
      "INFO:allennlp.training.trainer:Epoch duration: 0:00:00.039636\n",
      "INFO:allennlp.training.checkpointer:cannot load best weights without `serialization_dir`, so you're just getting the last weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 19,\n",
       " 'peak_cpu_memory_MB': 331.024,\n",
       " 'peak_gpu_0_memory_MB': 959,\n",
       " 'training_duration': '0:00:00.729805',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 19,\n",
       " 'epoch': 19,\n",
       " 'training_accuracy': 1.0,\n",
       " 'training_loss': 0.02861940860748291,\n",
       " 'training_cpu_memory_MB': 331.024,\n",
       " 'training_gpu_0_memory_MB': 959}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
